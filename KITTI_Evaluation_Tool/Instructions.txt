Typical evaluation pipeline

1. Get results using the object detector that you want to evaluate. Results must follow the KITTI format; 
please refer to the KITTI object detection devkit for further information. You should use a different set of frames 
for training and validation. As a result, you will have a folder with thousands of txt files, 
one per validation frame, and typically with non-correlative filenames.

2. Create a folder within eval_litti/build/results with the name of the experiment. Create a symbolic link to the folder 
containing your resulting txt files inside the newly created folder and rename it as data; e.g., 
if the experiment is named exp1, txt files to be evaluated should be accessible at eval_litti/build/results/exp1/data.

3. Go to eval_kitti/build and run evaluate_object with the name of the experiment and the txt file containing the
validation split as arguments. For instance, if you created the folder eval_litti/build/results/exp1 and are using
the valsplit validation set from here, you should run ./evaluate_object exp1 valsplit.

4. Wait until the script is completed. Modify lines 8 and 11 of eval_kitti/parser.py to specify the classes and parameters
for which you want average results; by default, you will obtain the same stats as in the KITTI benchmark. 
Then, go back to eval_kitti and run ./parser.py passing the name of the experiment as an argument. 
In our example, python parser.py exp1. Average stats (AP for detection, AOS for orientation, etc.) will be printed.